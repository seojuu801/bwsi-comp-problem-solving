{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99af76f2",
   "metadata": {},
   "source": [
    "# Final Project: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31f7f8",
   "metadata": {},
   "source": [
    "## Review of Regression\n",
    "\n",
    "Before we jump into today's holiday-themed activity, let's discuss **logistic regression**, a foundational model in supervised machine learning used for **classification** tasks.\n",
    "\n",
    "### What Problem Does Logistic Regression Solve?\n",
    "\n",
    "Unlike **linear regression**, which predicts a continuous value **logistic regression predicts a probability** that an observation belongs to a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614cf837",
   "metadata": {},
   "source": [
    "### Q: What are some examples that linear regression could be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1080",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73211d",
   "metadata": {},
   "source": [
    "### Q: What are some examples that logistic regression could be used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec40e96",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde4ad5",
   "metadata": {},
   "source": [
    "### Why Not Use a Line?\n",
    "\n",
    "If we tried using a normal linear regression line for classification, predictions could fall outside the range [0, 1], which doesn’t make sense for probabilities.\n",
    "\n",
    "To fix this, we apply a special mathematical function called the **sigmoid function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafed0c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306f98f",
   "metadata": {},
   "source": [
    "### The Sigmoid Function\n",
    "\n",
    "The sigmoid takes any real number and “squashes” it into a probability between 0 and 1: \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src = \"https://cdn.britannica.com/64/264764-050-A2C174FD/graph-of-a-sigmoid-function.jpg\" width = \"600\">\n",
    "</p>\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n$ \n",
    "- $\\beta$’s are model parameters (weights) (i.e the thing you're estimating)\n",
    "- $x$’s are input features (i.e the data you've collected)\n",
    "\n",
    "As $z \\to +\\infty$, output $\\to$ 1  \n",
    "As $z \\to -\\infty$, output $\\to$ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa888a8d",
   "metadata": {},
   "source": [
    "### Q: How do you think the model would perform if the relationship between the parameters was $z = \\beta_0 + \\beta_1 cos(x_1) + \\cdots + \\beta_n x_n^2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4b5ae",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f21649",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05c484",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "Once we have a probability, we turn it into a final prediction by applying a **threshold**:\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\sigma(z) \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For multi-class problems like today's (**naughty / nice / very nice**), we extend logistic regression using **softmax regression** (a type of multinomial logistic regression).\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax for Multiclass Classification\n",
    "\n",
    "Instead of outputting just one probability, the softmax function outputs **one probability per class**, and they always sum to 1. The probability that row index $i$ is of class $k$ is: \n",
    "\n",
    "$$\n",
    "P_i(y_i = k | x_i) = \\frac{e^{z_{i,k}}}{\\sum_{j=1}^{K} e^{z_{i,j}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z_k = \\beta_{0,k} + \\beta_{1,k}x_1 + ... + \\beta_{n,k}x_n$  \n",
    "- $K$ is the total number of classes  \n",
    "- Each class gets its own linear model output $z_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367e02",
   "metadata": {},
   "source": [
    "### Q: Say you have $n_k = 3$ classes and $n_f = 4$ features, how many model parameters does your model estimate? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e8b23",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8c75b",
   "metadata": {},
   "source": [
    "\n",
    "### How Does the Model Learn?\n",
    "\n",
    "Logistic regression parameters are learned by minimizing a cost function called **cross-entropy loss**, which measures how well the predicted probabilities match the actual outcomes.\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{ik} \\log(\\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "The computer updates parameters using an optimization method such as **gradient descent**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d7400",
   "metadata": {},
   "source": [
    "## Holiday Classification Problem\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://embroideres.com/files/1215/6749/7822/grinch_naughty_or_nice_machine_embroidery_design.jpg\" width = \"400\">\n",
    "</p>\n",
    "\n",
    "Santa wants a model to classify children into:\n",
    "- Naughty\n",
    "- Nice\n",
    "\n",
    "and eventually into:\n",
    "- Naughty\n",
    "- Nice\n",
    "- Very Nice\n",
    "\n",
    "You are given the following feature dataset:\n",
    "- good_deeds — number of good deeds this year\n",
    "- tantrums — number of tantrums\n",
    "- cookies_left — how many times they left cookies for Santa\n",
    "- chores_done — % of assigned chores completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cffb7c",
   "metadata": {},
   "source": [
    "### Part 1: Create Dataset\n",
    "First, we need to generate some data to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "N = 300 \n",
    "\n",
    "# sample features from different distributions; really the type of distribution isn't too important here\n",
    "good_deeds = np.random.poisson(10, N)\n",
    "tantrums = np.random.poisson(5, N)\n",
    "cookies_left = np.random.binomial(10, 0.4, N)\n",
    "chores_done = np.random.uniform(0, 1, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e7f22",
   "metadata": {},
   "source": [
    "Next, we will set **ground truth** weights so we can check if our model actually learned the underlying behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your weights and threshold for labeling the sample nice or not nice (naughty)\n",
    "# -- this is our back of the textbook solution \n",
    "weights_per_feature = {\n",
    "    \"good_deeds\": 1,\n",
    "    \"tantrums\": 1,\n",
    "    \"cookies_left\": 0.5,    \n",
    "    \"chores_done\": 5\n",
    "}\n",
    "threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d0e07",
   "metadata": {},
   "source": [
    "Now we create a single value that quantifies a person's overall niceness (let's call this `nice` for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a label based on a simple POLYNOMIAL function of the features \n",
    "nice = (weights_per_feature['good_deeds']*good_deeds\n",
    "        - weights_per_feature['tantrums']*tantrums\n",
    "        + weights_per_feature['chores_done']*chores_done \n",
    "        + weights_per_feature['cookies_left']*cookies_left > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f5eb7",
   "metadata": {},
   "source": [
    "Store all of this in a dataframe so we can test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package things into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"good_deeds\": good_deeds,\n",
    "    \"tantrums\": tantrums,\n",
    "    \"cookies_left\": cookies_left,\n",
    "    \"chores_done\": chores_done,\n",
    "    \"nice\": nice\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b94b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21275bfc",
   "metadata": {},
   "source": [
    "### Part 2: Check the Dataset\n",
    "\n",
    "Awesome! Let's train a `LogisticRegression` model and see if it can actually learn the rules we secretly set above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c2299",
   "metadata": {},
   "source": [
    "### Q: Fill in the code below to train and test a `LogisticRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your features\n",
    "#X = df[[       ???      ]]\n",
    "\n",
    "# predict if the person is nice\n",
    "# 0 = naughty, 1 = nice\n",
    "#y = df[ ? ]\n",
    "\n",
    "# split training and testing data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(   ,   , test_size=0.25, random_state=0)\n",
    "\n",
    "# train the model\n",
    "model = LogisticRegression()\n",
    "#model.fit(  ,   )\n",
    "\n",
    "# make predictions\n",
    "#y_pred = model.predict(    )\n",
    "\n",
    "# report the result of your model\n",
    "# you should see that your coefficients roughly match the weights you defined above \n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7a2b9",
   "metadata": {},
   "source": [
    "### Q: In this case, what might be a better metric than accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654694f2",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debf93b",
   "metadata": {},
   "source": [
    "### Q: Interpret the general impact of each coefficient on the class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0caff03",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd03278",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ca54a",
   "metadata": {},
   "source": [
    "### Part 3: Multiclass Naughty or Nice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46270705",
   "metadata": {},
   "source": [
    "Once again, we start by combining the feature data into a single score (now called `conditions`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute niceness scores -- notice it's still polynomial in the features\n",
    "conditions = (\n",
    "    weights_per_feature['good_deeds']*good_deeds \n",
    "    - weights_per_feature['tantrums']*tantrums \n",
    "    + weights_per_feature['chores_done']*chores_done \n",
    "    + weights_per_feature['cookies_left']*cookies_left\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7352a9",
   "metadata": {},
   "source": [
    "Now let's try a multiclass problem and sort into 3 classes: naughty, nice, and *very* nice.\n",
    "\n",
    "Here we define another truth set, but with multiple classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 3 classes: naughty (0), nice (1), very nice (2)\n",
    "class2label = {0: \"naughty\", 1: \"nice\", 2: \"very nice\"}\n",
    "y_multi = np.where(conditions < 5, 0, np.where(conditions < 12, 1, 2))\n",
    "df[\"multi_label\"] = y_multi\n",
    "\n",
    "# isolate features\n",
    "X = df[[\"good_deeds\", \"tantrums\", \"cookies_left\", \"chores_done\"]]\n",
    "\n",
    "# get labels\n",
    "y = df[\"multi_label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05dfe35",
   "metadata": {},
   "source": [
    "But what does this dataset even look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(conditions, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(5, color='red', linestyle='--', label='naughty/nice')\n",
    "plt.axvline(12, color='green', linestyle='--', label='nice/very nice')\n",
    "\n",
    "plt.title(\"Niceness Score Distribution with Class Boundaries\")\n",
    "plt.xlabel(\"Niceness Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef0233",
   "metadata": {},
   "source": [
    "### Q: Is it obvious that there are 3 underlying populations by eye?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba8d2d4",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12650a1",
   "metadata": {},
   "source": [
    "Let's try training another `LogisticRegression` model.\n",
    "\n",
    "### Q: Fill in the code below to train and test a `LogisticRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593aaa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(    ,   , test_size=0.25, random_state=0)\n",
    "\n",
    "# train the model \n",
    "#multi = LogisticRegression(max_iter=500)\n",
    "#multi.fit(   ,   )\n",
    "\n",
    "# test the model \n",
    "#y_pred = multi.predict(     )\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Coefficients:\", multi.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810859d7",
   "metadata": {},
   "source": [
    "### Q: Why do we have 12 coefficient estimates here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9852d",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7440e1",
   "metadata": {},
   "source": [
    "### Q: Do you think the model learned the underlying behavior? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d4d36",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f734c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, cmap='RdYlGn')\n",
    "plt.colorbar()\n",
    "\n",
    "# add numbers on cells\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "\n",
    "\n",
    "str_labels = [class2label[c] for c in range(3)]\n",
    "plt.xticks(range(3), str_labels)\n",
    "plt.yticks(range(3), str_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba812e9",
   "metadata": {},
   "source": [
    "### Q: What do you notice about the confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8a323",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c8c89",
   "metadata": {},
   "source": [
    "### Q: What about this dataset is causing this confusion matrix to be misleading?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a21503",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
